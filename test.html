
<!-- By Kayo Yin.
Built on https://startbootstrap.com/theme/grayscale.
Copyright Kayo Yin 2021.
All Rights Reserved.
Do not copy or distribute without permission of the author. -->


<!DOCTYPE html>
<html lang="en">
    <head>
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ1EQVYGEW"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-BJ1EQVYGEW');
      </script>
      
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Kayo Yin</title>
        <link rel="icon" type="image/x-icon" href="assets/bear.png" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />

        <script language="javascript">
            function show( elem ) {
                var x = document.getElementById(elem);
                if (x.style.display === "block") {x.style.display = "none";}
                else {x.style.display = "block";}
            }
        </script>
        <base target="_blank">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />


    </head>
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="#page-top" target="_self">Kayo Yin</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about" target="_self">About</a></li>
                        <!-- <li class="nav-item"><a class="nav-link" href="#research" target="_self">Research</a></li> -->
                        <li class="nav-item"><a class="nav-link" href="#pubs" target="_self">Publications</a></li>
                        <li class="nav-item"><a class="nav-link" href="#talks" target="_self">Talks</a></li>
                        <li class="nav-item"><a class="nav-link" href="#awards" target="_self">Awards</a></li>
                        <li class="nav-link disabled">|</li>
                        <!-- <li class="nav-item"><a class="nav-link" href="media.html" target="_self">Fun</a></li> -->
                        <li class="nav-item"><a class="nav-link" href="https://kayoyin.github.io/blog/" target="_self">Blog</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <div class="container d-flex align-items-center justify-content-center">
          <div class="d-flex justify-content-center">
        <div class="row">
          <div class="smallcol">
            <section class="about-section text-center" id="about">
                    <div class="text-center">
                       
                           <script type="text/javascript">
                                ImageArray = new Array();
                                ImageArray[0] = 'assets/img/flute.jpeg';
                                ImageArray[1] = 'assets/img/erhu.png';
                                ImageArray[2] = 'assets/img/piano.jpeg';
                                ImageArray[3] = 'assets/img/saxophone.png';
                            
                            function getRandomImage() {
                                var num = Math.floor( Math.random() * 4);
                                var img = ImageArray[num];
                                return('<img src="' + img + '" width="250px">')
                            
                            }
                            document.write(getRandomImage());
                            </script>
                            <br>
                            <h3 class="text-body mx-auto mt-2 mb-5">Hi, I'm Kayo <br>
                                Bonjour, je suis Kayo<br>
                                ‰ªäÊó•„ÅØ„ÄÅÁ∂∫Â¶§„Åß„Åô„ÄÄ<br>
                              </h3>
                                <tt>[kajo i…¥] </tt><br>
                                <tt>she/her </tt><br>
                                
                                <tt>kayoyinü•∏berkeley.edu</tt>
                                                 <br>
                                     
                                                 <a href="assets/cv.pdf"><tt>Vitae</tt></a> ‚ãÖ
                                                 <a href="bio.html" target="_self"><tt>Bio</tt></a> ‚ãÖ
                                                 <a href="media.html" target="_self"><tt>Fun</tt></a> <br>
                                                 
                                       <a href="https://scholar.google.com/citations?user=Wc8oLVwAAAAJ&hl=en"><img src="assets/img/scholar.svg" ></a>
                                       <!-- <a rel="me" 
                                       href="https://sigmoid.social/@k" title="Mastodon"><img src="assets/img/mastodon.svg" ></a> -->
                                       <a href="https://github.com/kayoyin" title="Github"><img src="assets/img/github.svg"></a>
                                       <a href="https://twitter.com/kayo_yin" title="Twitter"><img src="assets/img/twitter.svg" ></a>
                                       <a href="https://www.linkedin.com/in/kayoyin/" title="Linkedin"><img src="assets/img/linkedin.svg"></a>
                                       <a href="https://www.goodreads.com/user/show/32481095-kayo-yin" title="Goodreads"><img src="assets/img/goodreads.svg" ></a>
                                       <!-- <a href="https://www.strava.com/athletes/108038495" title="Strava"><img src="assets/img/strava.svg" ></a> -->
                                       <br><br>
                                       <a href="https://www.admonymous.co/kayoyin">Anonymous feedback</a>
                                       <!-- <a href="https://medium.com/@kayo.yin" title="Medium"><img src="assets/img/medium.svg" width="30" height="30"></a> <br> <br> -->

                                       <!-- <style type="text/css" media="screen">
                                        .gr_grid_container {
                                          /* customize grid container div here. eg: width: 500px; */
                                        }
                                
                                        .gr_grid_book_container {
                                          /* customize book cover container div here */
                                          float: left;
                                          width: 39px;
                                          height: 60px;
                                          padding: 0px 0px;
                                          overflow: hidden;
                                        }
                                      </style>
                                    
                                      <script src="https://www.goodreads.com/review/grid_widget/32481095.Kayo's%20bookshelf:%20read?cover_size=small&hide_link=true&hide_title=true&num_books=9&order=d&shelf=read&sort=date_read&widget_id=1662720503" type="text/javascript" charset="utf-8"></script>
                                 -->
                                     </ul>
                            
                            </div></section></div>
                           
                                <div class="bigcol">
                                  <section class="about-section" id="about">
                                    <div class="text-start">
                                      
                                      Hello! I'm a PhD student at <a href="https://www.berkeley.edu/">UC Berkeley</a> advised by <a href="https://jsteinhardt.stat.berkeley.edu">Jacob Steinhardt</a> and <a href="https://people.eecs.berkeley.edu/~klein/">Dan Klein</a>, and affiliated with <a href="https://bair.berkeley.edu/">Berkeley AI Research</a> and <a href="http://nlp.cs.berkeley.edu/">Berkeley NLP</a>. 
                                      I'm interested in better understanding LLMs to make them safe and robust, and in NLP for signed languages. I am grateful to be supported by a <a href="https://futureoflife.org/grant-program/phd-fellowships/">Future of Life PhD fellowship</a>. 
                                      <br><br>
                                    
                                    I did my master's at <a href="https://www.lti.cs.cmu.edu"> Carnegie Mellon University</a> where I was fortunate to be advised by <a href="http://www.phontron.com/">Graham Neubig</a>,
                                    and I did my undergrad at <a href="https://www.polytechnique.edu/en">√âcole Polytechnique</a>. 
                                    I also interned at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/">Microsoft Research</a> and <a href="https://www.deepmind.com/"> DeepMind</a>.
                                    In a previous life, I wanted to become a classical musician and I have a <a href="https://metiers.philharmoniedeparis.fr/etudes-conservatoire-musique.aspx">CEM</a> from <a href="https://conservatoires.paris.fr/conservatoires/chopin">Conservatoire Fr√©d√©ric Chopin</a>.
                                    
                              <br><br>
                               I come from Akashi, Japan and grew up in Paris, France. I like to play <a href="media.html">music</a>, practice martial arts, backcountry snowboard, and paint <a href="assets/img/cat.png">memes</a>. 
                                 <br><br>
                                 <div class="myBox">
                                  Upcoming events:
                                  <li>
                                    <b>2025-01-14</b> I will give an oral presentation at <a href="https://tislrethiopia.org/">TISLR</a> (Addis Ababa, Ethiopia).
                                 <br>
                                   </li>
                                   <li>
                                    <b>2024-12-14</b> I am co-organizing the <a href="https://solar-neurips.github.io/">SoLaR Workshop</a> at NeurIPS (Vancouver, Canada).
                                 <br>
                                   </li>
                                   <li>
                                    <b>2024-11-12~16</b> I will present 2 papers at <a href="https://2024.emnlp.org/">EMNLP</a> (Miami, FL).
                                 <br>
                                   </li>
                                  <li>
                                    <b>2024-10-23</b> I will give an invited talk (in Japanese) at <a href="https://nlp-colloquium-jp.github.io/">NLP„Ç≥„É≠„Ç≠„Ç¶„É†</a> (online).
                                 <br>
                                   </li>
                                   <br>
                                   Past news:
                                  <li>
                                    <b>2024-07-27</b> Co-organized the <a href="https://icml2024mi.pages.dev/">Mechanistic Interpretability Workshop</a> at ICML.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2024-06-19</b> Gave an invited talk at <a href="https://www.unimelb.edu.au/">University of Melbourne</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2024-05-02</b> Gave an invited talk at <a href="https://www.epfl.ch/en/">EPFL</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-10-27</b> Gave an invited talk at <a href="https://iid.ulaval.ca/">Universit√© Laval</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-07-10</b> Extremely thrilled to receive the <b>Best Resource Paper</b> award at <a href="https://2023.aclweb.org/">ACL 2023</a>!
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-05-15</b> I started my internship at Microsoft Research! Ping me if you want to meet up in NYC :)
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-04-28</b> Gave an invited talk at <a href="https://www.kungfu.ai/">KUNGFU.AI</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-04-26</b> Gave an invited talk at <a href="https://www.sonycsl.co.jp/">Sony CSL</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2023-02-10</b> Gave an invited talk at the <a href="https://www.uchicago.edu/">University of Chicago</a> and <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2022-12-19</b> Gave an invited talk at the <a href="https://www.unimelb.edu.au/">University of Melbourne</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2022-12-11</b> Extremely honored to receive the <b>Best Paper Honorable Mention</b> award at <a href="https://2022.emnlp.org/">EMNLP 2022</a>!
                                   </li>
                                  <li>
                                    <b>2022-08-19</b> Gave an invited talk at the <a href="https://christianhardmeier.rax.ch/workshop/pronouns-and-mt-2022/">Workshop on Pronouns and Machine Translation</a>.
                                 <br>
                                   </li>
                                  <li>
                                    <b>2022-07-27</b> Gave an invited presentation at IJCAI on <a href="https://aclanthology.org/2021.acl-long.570/">Including Signed Languages in NLP</a>. My first in-person conference yay! <br>
                                   </li>
                                   <li>
                                    <b>2022-07-09</b> Gave a keynote talk at the <a href="https://www.queerinai.com/naacl-2022">Queer in AI Workshop @NAACL</a>.<br>
                                   </li>
                                  <li>
                                    <b>2022-06-06</b> I started my internship at DeepMind! If you're in London this summer, let's meet up :) <br>
                                   </li>
                                  <li>
                                  <b>2022-05-19</b> Guested on the <a href="https://soundcloud.com/nlp-highlights">NLP Highlights Podcast</a>. <br>
                                 </li>
                                 <li>
                                  <b>2022-04-15</b> I will join <a href="https://bair.berkeley.edu">UC Berkeley</a> for my PhD next Fall! <br>
                                 </li>
                                 <li>
                                  <b>2021-11-05</b> Gave an invited talk at DeepMind on <a href="assets/slides/deepmind21.pdf">Natural Language Processing for Signed Languages</a> <br>
                                 </li>
                                 <li>
                                  <b>2021-10-07</b> Gave an invited talk at University of Pittsburgh on <a href="assets/slides/upitt21.pdf">Extending Neural Machine Translation to Dialogue and Signed Languages</a>
                                 </li>
                                 <li>
                                   <b>2021-09-23</b> Extremely honored to be selected as a <b><a href="https://www.siebelscholars.com/">Siebel Scholar</a> Class of 2022</b>!
                                 </li>
                                 
                                 <li>
                                  <b>2021-09-17</b> Gave an invited talk at SIGTYP on <a href="https://youtube.com/playlist?list=PLFIGad0NI4ougZCcNIJXUIW0XRnF-FXdJ">Understanding, Improving and Evaluating Context Usage in Context-aware Machine Translation</a>
                                 </li>
                                 <!-- <li>
                                  <b>2021-08-26</b> Super happy to have 2 papers accepted to <a href="https://2021.emnlp.org/">EMNLP 2021</a>!
                              </li> -->
                                 <!-- <li>
                                  <b>2021-07-25</b> 1 paper accepted to the <a href="https://sites.google.com/tilburguniversity.edu/at4svl2021/home?authuser=0">AT4SSL workshop</a> at MT Summit 2021!
                                 </li> -->
                                 <li>
                                  <b>2021-07-05</b> Extremely thrilled to receive the <b>Best Theme Paper</b> award at <a href="https://2021.aclweb.org/">ACL 2021</a>!
                                 </li>
                                 <!-- <li>
                                  <b>2021-05-06</b> Super excited to have 3 papers accepted to <a href="https://2021.aclweb.org/">ACL 2021</a>!
                              </li> -->
                                 <li>
                                  <b>2021-03-01</b> Gave an invited talk at Unbabel on <a href="assets/slides/unbabel21.pdf">Do Context-Aware Translation Models Pay the Right Attention?</a>
                              </li>
                                 <li>
                                  <b>2020-10-18</b> Gave an invited talk at Computer Vision Talks on <a href="https://youtu.be/E5nKeEvoAK0">Sign Language Translation with Transformers</a>
                               </li>
                               <!-- <li>
                                <b>2020-09-30</b> My first conference submission was accepted to <a href="https://coling2020.org/"> COLING'2020</a>!
                             </li> -->
                                 <li>
                                    <b>2020-09-21</b> Extremely honored to be awarded <b>Global Winner in Computer Science</b> at <a href="https://undergraduateawards.com/winners/global-winners-2020">The Global Undergraduate Awards 2020</a>!
                                 </li>
                                 <li>
                                  <b>2020-08-31</b>  Started my Master's degree at CMU LTI!
                               </li>
                               <!-- <li>
                                <b>2020-07-25</b>  My undergraduate paper has been accepted to the <a href="https://slrtp.com/">SLRTP workshop</a> at ECCV'20!
                             </li> -->
                            </div>
                                 </div>
                   
                     
            </section>
  
            <!-- <section class="about-section" id="research">
                <div class="text-start ">
                  <h6 class="display-6">Research</h6>
                <hr>
                My current research interests are as difficult to pin down as my music taste.<br>
                I am generally motivated by ideas that <ol>
                  <li>improve our understanding of how things work (e.g. black box models, natural languages, artificial/human intelligence, the universe...),</li>
                  <li>promote safety, inclusivity, and fairness,</li>
                  <li>become elegant solutions for really hard problems that will work well in the long run.</li>
                </ol>
              
                Here are some topics that I have worked on:<br> <br> 
                <li><b>Model Interpretability</b>: I am interested in explaining how large language models make decisions (<a href="https://arxiv.org/abs/2202.10419">EMNLP'22</a>) and understanding their capabilities and limitations. 
                  I am also interested in how neural networks process language on a fundamental level and how machine intelligence compares with human cognition.</li>
                <br> 
              
                <li><b>Sign Language Processing</b>: I am interested in modeling signed languages from a linguistic perspective and extending existing language technologies to signed languages (<a href="https://aclanthology.org/2021.acl-long.570/">ACL'21</a>, <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">ECCV'20</a>, <a href="https://www.aclweb.org/anthology/2020.coling-main.525/">COLING'20</a>, 
                  <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1/">MTSummit21</a>, <a href="https://aclanthology.org/2021.emnlp-main.405/">EMNLP'21</a>). I am also interested in using computational models to help us better understand how signed languages work. </li>  <br>
              
               <li><b>Context-aware Machine Translation</b>: I am interested in when context, either on an intra-sentential (within the current sentence), inter-sentential (across multiple sentences), or extra-linguistic (e.g. social, temporal, cultural) level, is required during translation, and how to model these features in machine translation 
                (<a href="https://aclanthology.org/2021.acl-long.505/">ACL'21</a>, <a href="https://aclanthology.org/2021.acl-long.65/">ACL'21</a>, <a href="https://arxiv.org/abs/2109.07446">ACL'23</a>). </li>
              
                <br><br>
                Please reach out if you'd like to chat or collaborate! I am generally responsive to emails and Twitter messages, I do not check LinkedIn very often, and I prefer that people do not contact me on social media that I have not listed on this website.
               </div>
            </section>  -->

            <section class="about-section" id="pubs">
                <div class="text-start">

                <h3 class="display-6">Publications</h3>
                <i>* = equal contribution</i>
                <br><br>

                <ul class="clist">
                  <li> 
                    ASL STEMWiki: Dataset and Benchmark for Interpreting STEM Articles
                    <br>
                    <b>Kayo Yin</b>,  Chinmay Singh, Fyodor O Minakov, Vanessa Milan, Hal Daum√© III, Cyril Zhang, Alex Xijie Lu, and Danielle Bragg.
                      <br>
                      <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2024.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs13')">
                        Abstract
                      </button>
                      <!-- <a href="https://arxiv.org/pdf/2406.04024"><span class="badge badge-pdf"> PDF </span></a> -->
                      <!-- <a href="https://github.com/kayoyin/asl-efficiency"><span class="badge badge-secondary">  Code </span></a> -->
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib13')">
                        BibTeX
                      </button>
                      <div id="abs13"  class="popup-abs">
                        &nbsp;&nbsp; Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEMWiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into 300 hours of American Sign Language (ASL). ASL STEMWiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL. We identify several use cases of ASL STEMWiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling in technical language, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled signs---which may later be used to query for appropriate ASL signs to suggest to interpreters.
                      </div>

                      <div id="bib13"  class="popup">
                        @inproceedings{yin24emnlp,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;title = {{ASL} {STEM}Wiki: Dataset and Benchmark for Interpreting {STEM} Articles},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;author = Yin, Kayo and Singh, Chinmay and O Minakov, Fyodor and Milan, Vanessa and Daum{'e} III, Hal and Zhang, Cyril and Xijie Lu, Alex and Bragg, Danielle<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;month = {November},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;year = {2024}<br>
                        }
                      </div><br><br>

                      <li> 
                        Using Language Models to Disambiguate Lexical Choices in Translation
                        <br>
                        Josh Barua, Sanjay Subramanian, <b>Kayo Yin</b>,  and Alane Suhr.
                          <br>
                          <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2024.</i>
                          
                          <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs12')">
                            Abstract
                          </button>
                          <!-- <a href="https://arxiv.org/pdf/2406.04024"><span class="badge badge-pdf"> PDF </span></a> -->
                          <!-- <a href="https://github.com/kayoyin/asl-efficiency"><span class="badge badge-secondary">  Code </span></a> -->
                            <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib12')">
                            BibTeX
                          </button>
                          <div id="abs12"  class="popup-abs">
                            &nbsp;&nbsp; In translation, a concept represented by a single word in a source language can have multiple variations in a target language. We introduce a dataset and evaluate language models on the task of lexical selection, which requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages, including seven low-resource languages, to collect a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on lexical selection, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4.
                          </div>
    
                          <div id="bib12"  class="popup">
                            @inproceedings{barua24emnlp,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;title = {Using Language Models to Disambiguate Lexical Choices in Translation},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;author = Barua, Josh and Subramanian, Sanjay and Yin, Kayo and Suhr, Alane<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;month = {November},<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;year = {2024}<br>
                            }
                          </div><br><br>

                  <li> 
                    <a href="https://arxiv.org/abs/2406.04024">American Sign Language Handshapes Reflect Pressures for Communicative Efficiency</a>
                    <br>
                    <b>Kayo Yin</b>,  Terry Regier, and Dan Klein.
                      <br>
                      <i>Conference of the Annual Meeting of the Association for Computational Linguistics (ACL). August 2024.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs11')">
                        Abstract
                      </button>
                      <a href="https://arxiv.org/pdf/2406.04024"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/asl-efficiency"><span class="badge badge-secondary">  Code </span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib11')">
                        BibTeX
                      </button>
        
                      <div id="abs11"  class="popup-abs">
                        &nbsp;&nbsp; Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality.
                        <br><br>
                        We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.
                      </div>

                      <div id="bib11"  class="popup">
                        @inproceedings{yin24acl,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;title = {Pressures for Communicative Efficiency in American Sign Language},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;author = {Yin, Kayo and Regier, Terry and Klein, Dan},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {Annual Conference of the Association for Computational Linguistics (ACL)},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;month = {August},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;year = {2024}<br>
                      }
                      </div><br><br>

                  <li> 
                    <span class="badge badge-sp"> üèÜ Best Resource Paper </span><br><a href="https://aclanthology.org/2023.acl-long.36/">When Does Translation Require Context? A Data-driven, Multilingual Exploration</a>
                    <br>
                    Patrick Fernandes*, <b>Kayo Yin*</b>,  Emmy Liu, Andr√© F. T. Martins and Graham Neubig.
                      <br>
                      <i>Conference of the Annual Meeting of the Association for Computational Linguistics (ACL). July 2023.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs9')">
                        Abstract
                      </button>
                      <a href="https://aclanthology.org/2023.acl-long.36.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://youtu.be/VqrSne-OuD4"> <span class="badge badge-video">Video</span></a>
                      <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">  Code </span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib9')">
                        BibTeX
                      </button>
        
                      <div id="abs9"  class="popup-abs">
                        &nbsp;&nbsp; Although proper handling of discourse phenomena significantly contributes to the quality
                        of machine translation (MT), common translation quality metrics do not adequately capture
                        them. Recent works in context-aware MT attempt to target a small set of these phenomena during evaluation. In this paper, we propose a new metric, P-CXMI, which allows us
                        to identify translations that require context systematically and confirm the difficulty of previously studied phenomena as well as uncover
                        new ones that have not been addressed in previous work. We then develop the Multilingual
                        Discourse-Aware (MuDA) benchmark, a series of taggers for these phenomena in 14 different language pairs, which we use to evaluate
                        context-aware MT. We find that state-of-theart context-aware MT models find marginal
                        improvements over context-agnostic models
                        on our benchmark, which suggests current
                        models do not handle these ambiguities effectively. We release code and data to invite
                        the MT research community to increase efforts
                        on context-aware translation on discourse phenomena and languages that are currently overlooked.
                        </div>

                      <div id="bib9"  class="popup">
                        @inproceedings{fernandes23acl,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;title = {When Does Translation Require Context? A Data-driven, Multilingual Exploration},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;author = {Patrick Fernandes and Kayo Yin and Emmy Liu and Andr√© Martins and Graham Neubig},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {Annual Conference of the Association for Computational Linguistics (ACL)},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;month = {July},<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;year = {2023}<br>
                      }
                      </div><br><br>

                <li> 
                  <span class="badge badge-sp"> üèÜ Best Paper Runner-Up </span><br><a href="https://aclanthology.org/2022.emnlp-main.14/"> Interpreting Language Models with Contrastive Explanations </a>
                  <br>
                  <b>Kayo Yin</b> and Graham Neubig.
                    <br>
                    <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). December 2022.</i>
                    
                    <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs10')">
                      Abstract
                    </button>
                    <a href="https://arxiv.org/pdf/2202.10419.pdf"><span class="badge badge-pdf"> PDF </span></a>
                    <a href="https://github.com/kayoyin/interpret-lm"> <span class="badge badge-secondary">Code</span></a>
                    <a href="https://youtu.be/tsAPyCsYTKs"> <span class="badge badge-video">Video</span></a>
                      <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib10')">
                      BibTeX
                    </button>
      
                    <div id="abs10"  class="popup-abs">
                      &nbsp;&nbsp; Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.
                      To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.
                      </div>

                    <div id="bib10"  class="popup">
                       @article{yin2022interpreting,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp; title = "Interpreting Language Models with Contrastive Explanations",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                          Neubig, Graham",<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)",<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;month = dec,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;year = "2022",<br>
                    }
                    </div><br><br>



                  <li> 
                    <a href="https://aclanthology.org/2021.emnlp-main.405/">Signed Coreference Resolution</a>
                    <br>
                    <b>Kayo Yin</b>, Kenneth DeHaan and Malihe Alikhani.
                      <br>
                      <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2021.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs8')">
                        Abstract
                      </button>
                
                      <a href="https://aclanthology.org/2021.emnlp-main.405.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/scr"> <span class="badge badge-secondary">Code</span></a>
                      <a href="https://youtu.be/jWAukIkICrs"> <span class="badge badge-video">Video</span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib8')">
                        BibTeX
                      </button>
        
                      <div id="abs8"  class="popup-abs">
                        &nbsp;&nbsp; Coreference resolution is key to many natural language processing tasks and yet has only been explored for spoken languages.
                        In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language.
                        In this paper, we: (1) introduce Signed Coreference Resolution, a new challenge for coreference modeling and Sign Language Processing; (2) collect an annotated corpus of German Sign Language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture, iconicity, and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively. Finally, we invite the NLP community to collaborate with signing communities and direct efforts towards SCR to close this gap.
                        </div>
                        
                        <div id="bib8"  class="popup">
                          @inproceedings{yin-etal-2021-signed,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;title = "Signed Coreference Resolution",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                                DeHaan, Kenneth  and
                                Alikhani, Malihe",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;month = nov,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;address = "Online and Punta Cana, Dominican Republic",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.emnlp-main.405",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;pages = "4950--4961",<br>
                          }
                      </div><br><br>

                  <li> 
                    <a href="https://aclanthology.org/2021.emnlp-main.553/">When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection</a>
                    <br>
                    Aditi Chaudhary, <b>Kayo Yin</b>, Antonios Anastasopoulos and Graham Neubig.
                      <br>
                      <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2021.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs7')">
                        Abstract
                      </button>
                      <a href="https://aclanthology.org/2021.emnlp-main.553.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/Aditi138/LexSelection"> <span class="badge badge-secondary">Code</span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib7')">
                        BibTeX
                      </button>
        
                      <div id="abs7"  class="popup-abs">
                        &nbsp;&nbsp; Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun ``wall'' has different lexical manifestations in Spanish --  ``pared'' refers to an indoor wall while ``muro'' refers to an outside wall.
                        However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way.
                        In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek,  where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations 
                        </div>

                      <div id="bib7"  class="popup">
                         @inproceedings{chaudhary21emnlp,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp; title = "When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection",<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;author = "Chaudhary, Aditi  and
                            Yin, Kayo  and
                            Anastasopoulos, Antonios and
                            Neubig, Graham",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;month = nov,<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                      }
                      </div><br><br>



                      <li> 
                        <span class="badge badge-sp"> üèÜ Best Theme Paper </span> <br><a href="https://aclanthology.org/2021.acl-long.570/">Including Signed Languages in Natural Language Processing</a> 
                      <br>
                      <b>Kayo Yin</b>, Amit Moryossef, Julie Hochgesang, Yoav Goldberg and Malihe Alikhani.
                        <br>
                        <i>Conference of the Annual Meeting of the Association for Computational Linguistics (ACL). August 2021. </i>
                        
                        <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs2')">
                          Abstract
                        </button>
                        <a href="https://aclanthology.org/2021.acl-long.570.pdf"><span class="badge badge-pdf"> PDF </span></a>
                        <a href="https://youtu.be/AYEIcOsUyWs"> <span class="badge badge-video">Video</span></a>
                          <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib2')">
                          BibTeX
                        </button>
          
                        <div id="abs2"  class="popup-abs">
                          &nbsp;&nbsp; Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.
                          </div>
                        
                        <div id="bib2"  class="popup">
                           @inproceedings{yin-etal-2021-including,<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;title = "Including Signed Languages in Natural Language Processing",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                              Moryossef, Amit  and
                              Hochgesang, Julie  and
                              Goldberg, Yoav  and
                              Alikhani, Malihe",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.570",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;pages = "7347--7360",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",<br>
                        }
                        </div><br><br>

                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.65/"> Do Context-Aware Translation Models Pay the Right Attention? </a>
                <br>
                <b>Kayo Yin</b>, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, Andr√© F. T. Martins and Graham Neubig.
                 <br>
                 <i>Conference of the Annual Meeting of the Association for Computational Linguistics (ACL). August 2021. </i>
                 <br>
                  <button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs1')">
                Abstract
              </button>
              <a href="https://aclanthology.org/2021.acl-long.65.pdf"><span class="badge badge-pdf"> PDF</span></a>
                  <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">  Code </span></a>
                  <a href="https://youtu.be/IRiy_xpWC-0"> <span class="badge badge-video">Video</span></a>
                <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib1')">
                BibTeX
              </button>

              <div id="abs1"  class="popup-abs">
                &nbsp;&nbsp; Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.
               </div>
              
              <div id="bib1"  class="popup">
                 @inproceedings{yin-etal-2021-context,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; title = "Do Context-Aware Translation Models Pay the Right Attention?",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; author = "Yin, Kayo  and
                    Fernandes, Patrick  and
                    Pruthi, Danish  and
                    Chaudhary, Aditi  and
                    Martins, Andr{\'e} F. T.  and
                    Neubig, Graham",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.65",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;pages = "788--801",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model{'}s attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.",<br>
              }
              </div><br><br>





                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.505/"> Measuring and Increasing Context Usage in Context-Aware Machine Translation </a>
                <br>
                  Patrick Fernandes, <b>Kayo Yin</b>, Graham Neubig and Andr√© F. T. Martins.
                 <br>
                  <i>Conference of the Annual Meeting of the Association for Computational Linguistics (ACL). August 2021. </i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs3')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.acl-long.505.pdf"><span class="badge badge-pdf"> PDF</span></a>
                      <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">Code</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib3')">
                    BibTeX
                  </button>
    
                  <div id="abs3"  class="popup-abs">
                    &nbsp;&nbsp; Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.
                    </div>
                  
                  <div id="bib3"  class="popup">
                     @inproceedings{fernandes-etal-2021-measuring,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;title = "Measuring and Increasing Context Usage in Context-Aware Machine Translation",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Fernandes, Patrick  and
                        Yin, Kayo  and
                        Neubig, Graham  and
                        Martins, Andr{\'e} F. T.",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.505",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "6467--6478",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",<br>
                  }
                  </div><br><br>


               <li> 
                <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1/"> Data Augmentation for Sign Language Gloss Translation</a>
                <br>
                Amit Moryossef*, <b>Kayo Yin*</b>,  Graham Neubig and Yoav Goldberg.
                  <br>
                  <i>Machine Translation Summit (MTSummit) International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL). August 2021.</i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs4')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1.pdf"><span class="badge badge-pdf"> PDF </span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib4')">
                    BibTeX
                  </button>
    
                  <div id="abs4"  class="popup-abs">
                    &nbsp;&nbsp; Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.
                    </div>
                  
                  <div id="bib4"  class="popup">
                     @inproceedings{moryossef-etal-2021-data,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp; title = "Data Augmentation for Sign Language Gloss Translation",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Moryossef, Amit  and
                        Yin, Kayo  and
                        Neubig, Graham  and
                        Goldberg, Yoav",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Virtual",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Machine Translation in the Americas",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.mtsummit-at4ssl.1",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "1--11",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on this synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.",
                  }
                  </div><br><br>

                 <li> 
                  <span class="badge badge-sp"> üèÜ Global Undergraduate Award </span> <br><a href="https://www.aclweb.org/anthology/2020.coling-main.525/">Better Sign Language Translation with STMC-Transformer</a>
                <br>
                <b>Kayo Yin</b> and Jesse Read.
                  <br>
                  <i>International Conference on Computational Linguistics (COLING). November 2020.</i> 

                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs5')">
                    Abstract
                  </button>
                  <a href="https://www.aclweb.org/anthology/2020.coling-main.525.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib5')">
                    BibTeX
                  </button>
    
                  <div id="abs5"  class="popup-abs">
                    &nbsp;&nbsp; Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU. We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.
                </div>
                  
                  <div id="bib5"  class="popup">
                     @inproceedings{yin-read-2020-better, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title = "Better Sign Language Translation with {STMC}-Transformer", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and Read, Jesse", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 28th International Conference on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = December, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2020", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Barcelona, Spain (Online)",
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "International Committee on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://www.aclweb.org/anthology/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;doi = "10.18653/v1/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "5975--5989", <br>
                    }
                  </div> <br><br>

                 <li> <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">
                  Sign Language Translation with Transformers
                  </a>
                <br>
                <b>Kayo Yin</b> and Jesse Read.
                 <br>
                  <i>European Conference on Computer Vision (ECCV) Workshop on Sign Language Recognition, Translation and Production (SLRTP). August 2020.</i> 
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs6')">
                    Abstract
                  </button>
                  <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code</span></a>
                      <a href="https://youtu.be/c-87jFd2lQs"> <span class="badge badge-video">Video</span></a>

                   
                      <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib6')">
                    BibTeX
                  </button>
    
                  <div id="abs6"  class="popup-abs">
                    &nbsp;&nbsp; This paper improves the translation system in Sign Language
                    Translation (SLT) by using Transformers. We report a wide range of experimental results for various Transformer setups and introduce a novel
                    end-to-end SLT system combining Spatial-Temporal Multi-Cue (STMC)
                    and Transformer networks. Our methodology improves on the current
                    state-of-the-art by over 5 and 7 BLEU respectively on ground truth
                    (GT) glosses and predicted glosses of the PHOENIX-Weather 2014T
                    dataset. On the ASLG-PC12 corpus, we report an improvement of over
                    16 BLEU. Our findings also reveal that end-to-end translation with predicted glosses outperforms translation on GT glosses. This shows the
                    potential for further improvement in SLT by either jointly training the
                    SLR and translation systems or by revising the gloss annotation scheme. </div>
                  
                  <div id="bib6"  class="popup">
                     @inproceedings{yin2020attention,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title={{Attention is All You Sign: Sign Language Translation with Transformers}},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author={Yin, Kayo and Read, Jesse},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Sign Language Recognition, Translation and Production (SLRTP) Workshop-Extended Abstracts},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;volume={4},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year={2020}<br>
                      }
                    </ul>
                    </section>
        
              <section id="talks" class="mt-5">
                <div class="text-start">
                <h3 class="display-6">Talks</h3>
                <hr>

                
                  <h4>2023</h4>
                  <ul class="clist">
                    <li>   <span class="badge badge-video">Talk</span> 
                      <a href="https://iid.ulaval.ca/evenements/webinaire-linclusion-des-langues-des-signes-dans-le-traitement-du-langage-naturel/">L'inclusion des langues des signes dans le traitement du langage naturel</a> (<a href="https://www.ulaval.ca/">Universit√© Laval</a>)
                    </li> 
                    <br>
                    <li>   <span class="badge badge-video">Talk</span> 
                      <a href="assets/slides/melb22.pdf">Interpreting Language Models with Contrastive Explanations</a> (<a href="https://www.kungfu.ai/">KUNGFU.AI</a>)
                    </li> 
                    <br>
                    <li> <span class="badge badge-video">Talk</span> 
                      <a href="assets/slides/ttic23.pdf">Natural Language Processing for Signed Languages</a> (<a href="https://www.sonycsl.co.jp/">Sony CSL</a>)
                    </li> 
                    <br>
                  <li> <span class="badge badge-video">Talk</span> 
                    <a href="assets/slides/ttic23.pdf">Natural Language Processing for Signed Languages</a> (<a href="https://www.uchicago.edu/">University of Chicago</a> & <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago</a>)
                  </li> 
                  <br>
                </ul>
                  <h4>2022</h4>
                  <ul class="clist">
                  <li>   <span class="badge badge-video">Talk</span> 
                    <a href="assets/slides/melb22.pdf">Interpreting Language Models with Contrastive Explanations</a> (<a href="https://www.unimelb.edu.au/">University of Melbourne</a>)
                  </li> 
                  <br>
                  <li>   <span class="badge badge-video">Talk</span> 
                    <a href="https://youtu.be/A6TX9SiOxiQ">Understanding, Improving and Evaluating Context Usage in Context-aware Translation</a> (<a href="https://christianhardmeier.rax.ch/workshop/pronouns-and-mt-2022">Workshop on Pronouns and Machine Translation</a>)

                  </li> 
                  <br>

                  <li>   <span class="badge badge-video">Talk</span> 
                    <a href="https://youtu.be/HZwkAGzxqm0">Queer Impostor Syndrome</a> (<a href="https://sites.google.com/view/queer-in-ai/naacl-2022">Queer in AI Workshop @ NAACL</a>)

                  </li> 
                  <br>
                  
                  <li>   <span class="badge badge-video">Talk</span> 
                    <a href="https://soundcloud.com/nlp-highlights/136-including-signed-languages-in-nlp-with-kayo-yin-and-malihe-alikhani?utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing">Including Signed Languages in NLP</a> (<a href="https://soundcloud.com/nlp-highlights">NLP Highlights</a>)
                  </li> 
                  <br>
                  
                  <li>   <span class="badge badge-secondary">Interview</span> 
                  <a href="https://magazine.cs.cmu.edu/studentspotlightkayoyin">Student Spotlight: Kayo Yin</a>
                </li> 
                <br>
              </ul>
              <h4>2021</h4>
                <ul class="clist">

                
                  <li>  <span class="badge badge-video">Talk</span> 
                      <a href="assets/slides/deepmind21.pdf">Natural Language Processing for Signed Languages</a> (<a href="https://deepmind.com/">DeepMind</a>)
                       </li> 
                       <br>

                  <li>  <span class="badge badge-secondary">Interview</span> 
                      <a href="https://www.jeanninemanuelalumni.org/news/2021/10/26/kayo-yin-class-of-2017"> Alumni Stories: Kayo Yin, Class of 2017</a>  
                       </li> 
                       <br>

                        <li>  <span class="badge badge-video">Talk</span> 
                            <a href="assets/slides/upitt21.pdf">Extending Neural Machine Translation to Dialogue and Signed Languages</a> (<a href="https://www.pitt.edu/">University of Pittsburgh</a>)
                             </li> 
                             <br>
                  <li> <span class="badge badge-video">Talk</span> 
                    <a href="https://youtu.be/dgCjT0M7Osc">Understanding, Improving and Evaluating Context Usage in Context-aware Machine Translation</a> (<a href="https://sigtyp.github.io/">SIGTYP</a>)
       
      </li> 
      <br>
                  <li>  <span class="badge badge-secondary">Interview</span> 
                      <a href="https://www.cs.cmu.edu/news/2021/nlp-signed-languages"> LTI Master's Student Urges NLP Focus on Signed Languages</a>  
                       </li> 
                       <br>

                        <li>  <span class="badge badge-video">Talk</span> 
                            <a href="assets/slides/unbabel21.pdf">Do Context-Aware Translation Models Pay the Right Attention?</a> (<a href="https://unbabel.com/">Unbabel</a>)
                             </li> 
                             <br>
                            </ul>
                            <h4>2020</h4>
                             <ul class="clist">
        
            <li>  <span class="badge badge-video">Talk</span> 
              <a href="https://youtu.be/MPit0Oh4reM">Sign Language Translation with Transformers</a> (<a href="https://undergraduateawards.com/">UA Global Summit</a>)
 
            
</li> 
<br>

    <li>   <span class="badge badge-secondary">Interview</span> 
        <a href="https://programmes.polytechnique.edu/en/post/bachelor-of-science-alumni-kayo-at-carnegie-mellon-university"> Bachelor of Science Alumni - Kayo at Carnegie Mellon University</a>  
         </li> 
         <br>
            <li>  <span class="badge badge-video">Talk</span> 
              <a href="https://youtu.be/E5nKeEvoAK0">Sign Language Translation with Transformers</a> (<a href="https://www.youtube.com/channel/UCseJlTlqQ2jfW66r-fOYaNg/videos">Computer Vision Talks</a>)
 
            
</li>   <br>
            <li>   <span class="badge badge-secondary">Interview</span> 
                <a href="https://www.polytechnique.edu/en/content/graduate-lx-bachelor-science-kayo-yin-wins-2020-global-undergraduate-awards"> Graduate of l‚ÄôX Bachelor of Science, Kayo Yin wins the 2020 Global Undergraduate Awards</a>  
         </li> 
         <br>
            <li>   <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/testimony-yin-kayo-bx2020-mathscomputer-science-student"> Testimony: Kayo Yin, BX2020 Maths/Computer Science Student</a>  
         </li> 
         <br>
        </ul>
        <h4>2017-2019</h4>
         <ul class="clist">
            <li>   <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/women-in-science-meet-kayo-yin-bachelor-student">Women in Science - Meet a student from the Bachelor Program </a>  
         </li> 
         <br>
            <li>  <span class="badge badge-secondary">Interview</span> 
              <a href="https://youtu.be/RoZAu4pHCxg">Meet our students - Kayo Yin</a> </li> 
           
         <br>
          <li>  <span class="badge badge-secondary">Interview</span> 
         <a href="https://gargantua.polytechnique.fr/siatel-web/linkto/mICYYYTGHYK#page=20t">A Lifetime of Science - Kayo Yin, Femme-Orchestre </a>  
         </li>
         </ul>

        </div>

            </section>
            <section id="awards" class="mt-5">
              <div class="text-start">
              <h3 class="display-6">Selected Awards</h3>
              <hr>

              <ul class="clist">
                <li> 
                  <span class="badge badge-abs">2023</span> Best Resource Paper, <a href="https://2023.aclweb.org/program/best_papers/">ACL</a> 
                </li> 
                <li> 
                  <span class="badge badge-abs">2023-2027</span> <a href="https://futureoflife.org/grant-program/phd-fellowships/">Future of Life Fellowship</a>
                   </li> 
                <li> 
                  <span class="badge badge-abs">2022</span> Best Paper Honorable Mention, <a href="https://arxiv.org/abs/2202.10419">EMNLP</a> 
                </li> 
                <li> 
                  <span class="badge badge-abs">2022-2023</span> <a href="https://grad.berkeley.edu/admissions/steps-to-apply/costs-fees/fellowships-entering/">Berkeley Fellowship</a>
                   </li> 
                <li> 
                  <span class="badge badge-abs">2021-2022</span> <a href="https://www.siebelscholars.com/scholar-profile/1994/">Siebel Scholarship</a>
                   </li> 
                   <li> 
                    <span class="badge badge-abs">2021</span> Best Theme Paper, <a href="https://2021.aclweb.org/program/accept/#best-theme-paper">ACL</a> 
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2020-2022</span> Carnegie Mellon University Research Fellowship
                     </li> 
                  <li> 
                    <span class="badge badge-abs">2020</span> Global Winner, <a href="https://undergraduateawards.com/winners/global-winners-2020">The Global Undergraduate Awards</a> 
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2015</span> Gold medal, <a href="http://www2.mathkang.org/default.html">Concours Kangourou des Math√©matiques</a> (6th place out of 13011)
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2012</span> Gold medal, <a href="http://www2.mathkang.org/default.html">Concours Kangourou des Math√©matiques</a> (5th place out of 53937)
                  </li> 
                    </ul>   
                  </div>
          
                </section>
          </div></div>      </div>
                        
        </div>
              
            <br>
            <p style="color:#808080; font-size:16px;" align="center" >Copyright ¬© Kayo Yin 2021-2024
              <script>
                document.addEventListener("DOMContentLoaded", function() {
                    var lastUpdatedElement = document.getElementById("last-updated");
                    var lastUpdatedDate = new Date(document.lastModified);
                    var options = { year: 'numeric', month: 'long', day: 'numeric' };
                    lastUpdatedElement.textContent = lastUpdatedDate.toLocaleDateString(undefined, options);
                });
            </script>
              <br> Last updated <span id="last-updated">January 1, 2024</span></p>

              <audio id="audio" src="assets/harp.mp3"></audio>
              <script>
                Array.prototype.random = function () {
  return this[Math.floor((Math.random()*this.length))];
}
              let audio = document.getElementById("audio");
              audio.volume = 0.3;
                var start = [3, 775, 1008, 1240, 1917].random()
                audio.currentTime = start;
                function play() {
                //   var starttimes = [
                //   3, 210, 488, 760, 998, 1229, 1486, 1756, 1910, 2180
                // ];
                //   var start = _.sample(starttimes);
                  return audio.paused ? audio.play() : audio.pause();
                }
              </script>

<div class="totoro">
            <div class="imglink">
              <div style="position:fixed; bottom:20px; right:10px;"><img src="assets/totorobottle.gif" height="100%" onclick="play()" title="‚ô™"></div>
            </div></div>
              <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
